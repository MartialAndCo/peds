# üî¥ RAPPORT D'ANALYSE - BUGS SWARM CRITIQUES

> **Date**: 2026-02-13  
> **S√©v√©rit√©**: CRITIQUE  
> **Impact**: >50% des r√©ponses contiennent des erreurs

---

## üì∏ BUGS OBSERV√âS EN PRODUCTION

### Bug 1: R√©p√©titions Massives
```
User: "Love can i see more photos of u??"
AI:   "Be patient, love. More soon. I'm always here for you."

User: "Ohh okay but i waan see u more"
AI:   "Be patient, love. More soon. I'm always here for you."

User: "Okay"
AI:   "Be patient, love. More soon. I'm always here for you."
```
**Fr√©quence**: 5+ occurrences identiques dans la m√™me conversation

### Bug 2: Messages Tronqu√©s
```
AI: "********** Les autres ont des iPhone 15 moi"
        ‚Üë se termine sur "moi" (pronom incomplet)
```

### Bug 3: Formatting Artifacts
```
AI: "**"  (seul, sans texte)
AI: "```"  (backticks sans code)
```

### Bug 4: Perte de Contexte
```
User: "Je suis ko"
User: "Et toi pas trop fatique"
User: "Fatigue"
AI:   "**"  (aucun rapport avec la fatigue)
```

---

## üîç CAUSES RACINES IDENTIFI√âES

### CAUSE #1: Max Tokens Trop Bas (CRITIQUE)
**Fichier**: `lib/swarm/nodes/response-node.ts:158`
```typescript
max_tokens: 50  // TROP BAS!
```

**Impact**:
- 50 tokens = ~40 mots maximum
- Les phrases sont coup√©es mid-sentence
- "Les autres ont des iPhone 15 moi" devait continuer avec "je n'ai que √ßa" ou similaire

**Solution**: Augmenter √† 100-150 tokens

---

### CAUSE #2: Temperature Trop Haute (CRITIQUE)
**Fichier**: `lib/swarm/nodes/response-node.ts:157`
```typescript
temperature: 0.7  // TROP CR√âATIF!
```

**Impact**:
- 0.7 = haute cr√©ativit√© = r√©p√©titions et incoh√©rences
- Le mod√®le "r√©invente" les m√™mes phrases
- Pas de coh√©rence avec l'historique

**Solution**: Baisser √† 0.3-0.4 pour plus de coh√©rence

---

### CAUSE #3: Frequency Penalty Trop Faible (HAUTE)
**Fichier**: `lib/venice.ts:54`
```typescript
frequency_penalty: config.frequency_penalty ?? 0.3  // TROP FAIBLE
```

**Impact**:
- 0.3 n'emp√™che pas les r√©p√©titions
- Le mod√®le r√©utilise "Be patient", "love", etc.

**Solution**: Augmenter √† 0.5-0.7

---

### CAUSE #4: Validator Ineffectif (CRITIQUE)
**Fichier**: `lib/swarm/nodes/validation-node.ts`

**Probl√®mes**:
1. **Conflit d'int√©r√™t**: Venice valide ses propres r√©ponses
2. **Aucune d√©tection programmatique**: Tout passe par LLM
3. **Seuil trop haut**: Confidence > 0.75 pour alerter
4. **Non bloquant**: M√™me si erreur d√©tect√©e, la r√©ponse part

**Code probl√©matique**:
```typescript
// Le validator demande √† Venice de valider... Venice
const validation = await venice.chatCompletion(validationPrompt, ...)
```

---

### CAUSE #5: Pas de M√©moire des Erreurs par Conversation (HAUTE)

**Probl√®me**:
- Aucun suivi des phrases d√©j√† utilis√©es
- Aucune "blacklist temporaire"
- Le syst√®me ne sait pas qu'il a d√©j√† dit "Be patient" 3x

**Donn√©es manquantes**:
```typescript
conversationMemory = {
  usedPhrases: [],      // Non impl√©ment√©
  errorStreak: 0,       // Non impl√©ment√©
  forbiddenPatterns: [] // Non impl√©ment√©
}
```

---

### CAUSE #6: Supervisor Asynchrone Non-Bloquant (HAUTE)
**Fichier**: `lib/handlers/chat.ts:871-899`

```typescript
// Analyse asynchrone (non-bloquante)
supervisorOrchestrator.analyzeResponse(supervisorContext).catch((err: any) => {
    console.error('[Chat] Supervisor analysis failed:', err)
})
// La r√©ponse est envoy√©e APR√àS ce bloc, sans attendre l'analyse!
```

**Impact**:
- Le supervisor d√©tecte les erreurs... apr√®s coup
- La r√©ponse est d√©j√† envoy√©e √† l'utilisateur
- Les alertes sont "pour info" uniquement

---

### CAUSE #7: Historique Court dans le Prompt (MEDIUM)
**Fichier**: `lib/swarm/nodes/response-node.ts:149`
```typescript
history.slice(-30)  // Seulement 30 messages
```

**Impact**:
- Avec 50 messages de log, on garde peu de contexte
- Les patterns de r√©p√©tition ne sont pas visibles

---

### CAUSE #8: Style Node √âcrase les R√®gles (MEDIUM)
**Fichier**: `lib/swarm/nodes/response-node.ts:93-95`
```typescript
// 6. Style additionnel depuis DB (si pr√©sent et diff√©rent)
if (contexts.style && contexts.style.length > 20) {
    promptParts.push(contexts.style)  // AJOUTE au lieu de FUSIONNER
}
```

**Impact**:
- Les r√®gles de style sont dupliqu√©es
- Confusion pour le mod√®le

---

## üìä M√âTRIQUES DE D√âFAILLANCE

| Type d'Erreur | Occurrences | % du Total |
|--------------|-------------|------------|
| R√©p√©titions | 5 | 42% |
| Artifacts (**```) | 3 | 25% |
| Troncatures | 2 | 17% |
| Perte contexte | 2 | 17% |
| **TOTAL** | **12** | **100%** |

**Taux de d√©faillance observ√©**: ~70% (7 erreurs sur 10 r√©ponses)

---

## üéØ RECOMMANDATIONS PRIORITAIRES

### üî• PRIORIT√â 1: Corrections Imm√©diates (Hotfix)

1. **Augmenter max_tokens**
   ```typescript
   max_tokens: 150  // Au lieu de 50
   ```

2. **Baisser temperature**
   ```typescript
   temperature: 0.4  // Au lieu de 0.7
   ```

3. **Augmenter frequency_penalty**
   ```typescript
   frequency_penalty: 0.6  // Au lieu de 0.3
   ```

### üî• PRIORIT√â 2: D√©tection Programmatique (Validation Node)

Ajouter AVANT l'appel LLM:
```typescript
// D√©tection de r√©p√©tition exacte
const lastAiResponses = history
  .filter(m => m.role === 'ai')
  .slice(-3)
  .map(m => m.content)

if (lastAiResponses.every(r => r === response)) {
    return { error: 'EXACT_REPEAT', regenerate: true }
}

// D√©tection de troncature
const truncationPatterns = /\b(moi|je|tu|il|elle|et|ou)\s*$/i
if (truncationPatterns.test(response)) {
    return { error: 'TRUNCATED', regenerate: true }
}

// D√©tection d'artifacts
if (/^\*+$/.test(response) || response.length < 3) {
    return { error: 'ARTIFACT', regenerate: true }
}
```

### üî• PRIORIT√â 3: M√©moire Conversationnelle

```typescript
// Dans la DB ou Redis
interface ConversationMemory {
    conversationId: string
    usedPhrases: string[]      // Hash des derni√®res phrases
    phraseCount: Record<string, number>  // Compteur par phrase
    errorStreak: number
    lastValidResponse: string
}

// Avant g√©n√©ration, injecter:
const forbiddenPhrases = memory.usedPhrases
  .filter(p => memory.phraseCount[p] > 2)

prompt += `\n‚ö†Ô∏è INTERDIT de dire: ${forbiddenPhrases.join(', ')}`
```

### üî• PRIORIT√â 4: Validation Bloquante

Rendre le supervisor bloquant:
```typescript
// AU LIEU DE:
supervisorOrchestrator.analyzeResponse(context)  // fire & forget

// FAIRE:
const validation = await supervisorOrchestrator.validateBlocking(context)
if (!validation.isValid) {
    return regenerateWithConstraints(validation.issues)
}
```

---

## üß™ TESTS DE REPRODUCTION

Les tests complets sont dans: `tests/swarm-bug-analysis.test.ts`

Ex√©cuter:
```bash
npm test -- tests/swarm-bug-analysis.test.ts
```

---

## üìã CHECKLIST DE CORRECTION

- [ ] Augmenter max_tokens √† 150
- [ ] Baisser temperature √† 0.4
- [ ] Augmenter frequency_penalty √† 0.6
- [ ] Ajouter d√©tection de r√©p√©tition exacte (validation-node)
- [ ] Ajouter d√©tection de troncature (validation-node)
- [ ] Ajouter d√©tection d'artifacts (validation-node)
- [ ] Impl√©menter m√©moire conversationnelle
- [ ] Rendre validation bloquante
- [ ] Ajouter tests de r√©gression

---

## üîÆ LONG TERME: Architecture Sentinel

Une fois les hotfixes d√©ploy√©s, envisager le syst√®me Sentinel:
- Cache des erreurs par conversation
- Validator externe (Claude Haiku) pour validation crois√©e
- Meta-learning des patterns d'erreur
- Auto-correction incr√©mentale

---

**Rapport g√©n√©r√© par**: Kimi Code CLI  
**Bas√© sur**: Analysis du code swarm + captures d'√©cran utilisateur
